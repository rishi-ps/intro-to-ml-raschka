{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. k-Nearest Neighbour "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "1. Decision boundary diagram == Voronoi diagram / Voronoi tesselation\n",
    "\n",
    "2.  **Continuous distance measures:**\n",
    "    - **Minkowski** distance measure = $ d(\\mathbf{x}^{[a]}, \\mathbf{x}^{[b]}) = \\left( \\sum_{j=1}^{m} |x_j^{[a]} - x_j^{[b]}|^p \\right)^{\\frac{1}{p}} $ \n",
    "\n",
    "        where,  \n",
    "        $m$ = number of features  \n",
    "        $\\mathbf{x^{[a]}}$ = $a^{th}$ feature vector\n",
    "\n",
    "        when $p$ = 1, it is called **Manhattan** distance  \n",
    "        when $p$ = 2, it is called **Euclidean** distance  \n",
    "\n",
    "        Side note: **\"Mahalanobis distance\"** = Euclidean distance, when features are not related to each other.\n",
    "    \n",
    "    <br>\n",
    "\n",
    "    - **Important note:** \n",
    "        \n",
    "        We want to have features on the **\"same scale\"(range)** before measuring the distances. If not, the feature having larger values will dominate the distance calculation and the kNN algorithm will mostly decide \"closeness\" based on this feature alone. If this feature is an important one, then it's fine. If not, then we would be using a poor feature to classify/regress. \n",
    "\n",
    "        Mahalanobis distance accounts for differences in scale and correlation among features when computing distances.\n",
    "\n",
    "    <br>\n",
    "\n",
    "    -   Another useful distance measure is the **Cosine similarity** (measures angle between 2 feature vectors).\n",
    "\n",
    "        Cosine similarity = $ \\cos {\\theta} = \\frac{\\mathbf{a}.\\mathbf{b}}{a.b}$\n",
    "\n",
    "        **Example**: Consider the case of classifying documents. Suppose initially we have two duplicate documents. The Euclidean distance is going to be 0 and the Cosine similarity is going to be 1. Now, if we add a few duplicate words in one of the documents, the Cosine distance between the two documents will remain 1 because the angle between the \"document vectors\" remains the same, only the length of one of the \"document vector\" increases. However, the Euclidean distance will not remain 0 anymore because the value of certain features has increased for one of the \"document vectors\". Hence, the subtraction of feature values will not result in 0 anymore.\n",
    "\n",
    "    <br>\n",
    "\n",
    "    **Discrete distance measures:**\n",
    "\n",
    "    - **Hamming distance**: $ d(\\mathbf{x}^{[a]}, \\mathbf{x}^{[b]}) = \\sum_{j=1}^{m}|x_j^{[a]} - y_j^{[b]}| $, where $x_j \\in {\\{0, 1\\}}$, i.e binary feature vectors.\n",
    "\n",
    "    - **Jaccard/Tanimoto similarity**: ${J}(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}$  \n",
    "      **Jaccard distance** = 1 - Jaccard Similarity\n",
    "\n",
    "    **Feature scaling**: Always scale features before using distance-based methods. Two problems arise if features are not in the same scale (range):\n",
    "    1. Feature having larger values will dominate distance calculation (as mentioned above).\n",
    "    2. The similarity changes in different scales (as can be seen in the below plots).\n",
    "\n",
    "    <img src=\"images/feature-scaling.png\" alt=\"feature-scaling\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
